apiVersion: v1
data:
  # stroppy
  password: c3Ryb3BweQ==
  username: c3Ryb3BweQ==
kind: Secret
metadata:
  labels:
    application: spilo
    cluster-name: acid-postgres-cluster
    team: acid
  name: stroppy.acid-postgres-cluster.credentials.postgresql.acid.zalan.do
type: Opaque
---
apiVersion: "acid.zalan.do/v1"
kind: postgresql
metadata:
  name: acid-postgres-cluster
#  labels:
#    environment: demo
#  annotations:
#    "acid.zalan.do/controller": "second-operator"
#    "delete-date": "2020-08-31"  # can only be deleted on that day if "delete-date "key is configured
#    "delete-clustername": "acid-test-cluster"  # can only be deleted when name matches if "delete-clustername" key is configured
spec:
  dockerImage: registry.opensource.zalan.do/acid/spilo-13:2.0-p7
  teamId: "acid"
  numberOfInstances: 3
  users:  # Application/Robot users
    stroppy:
    - superuser
    - createdb
  enableMasterLoadBalancer: false
  enableReplicaLoadBalancer: false
  enableConnectionPooler: false # enable/disable connection pooler deployment
  enableReplicaConnectionPooler: false # set to enable connectionPooler for replica service
  allowedSourceRanges:  # load balancers' source ranges for both master and replica services
  - 127.0.0.1/32
  databases:
    stroppy: stroppy
  preparedDatabases:
    bar:
      defaultUsers: true
      extensions:
        pg_partman: public
        pgcrypto: public
      schemas:
        data: {}
        history:
          defaultRoles: true
          defaultUsers: false
  postgresql:
    version: "13"
    parameters:
      # Connectivity
      max_connections: '100'
      superuser_reserved_connections: '3'
      # Memory Settings
      shared_buffers: '2048 MB'
      work_mem: '32 MB'
      maintenance_work_mem: '620 MB'
      huge_pages: 'off'
      effective_cache_size: '6 GB'
      effective_io_concurrency: '100'
      random_page_cost: '1.25'
      # Monitoring
      shared_preload_libraries: 'pg_stat_statements'
      track_io_timing: 'on'
      track_functions: pl
      # Replication
      wal_level: replica
      max_wal_senders: '10'
      synchronous_commit: 'on'
      # Checkpointing: 
      checkpoint_timeout : '15 min' 
      checkpoint_completion_target: '0.9'
      max_wal_size: '10240 MB'
      min_wal_size: '5120 MB'
      # WAL archiving
      archive_mode: 'on' # having it on enables activating P.I.T.R. at a later time without restart
      archive_command: '/bin/true'  # not doing anything yet with WAL-s
      # WAL writing
      wal_compression: 'on'
      wal_buffers: '-1'
      wal_writer_delay: 200ms
      wal_writer_flush_after: 1MB
      wal_keep_size: '22080 MB'
      # Background writer
      bgwriter_delay: 200ms
      bgwriter_lru_maxpages: '100'
      bgwriter_lru_multiplier: '2.0'
      bgwriter_flush_after: '0'
      # Parallel queries: 
      max_worker_processes: '4'
      max_parallel_workers_per_gather: '2'
      max_parallel_maintenance_workers: '2'
      max_parallel_workers: '4'
      parallel_leader_participation: 'on'
      # Advanced features 
      enable_partitionwise_join: 'on'
      enable_partitionwise_aggregate: 'on'
      jit: 'off'
  volume:
    size: 500Gi
    storageClass: local-path
#    iops: 1000  # for EBS gp3
#    throughput: 250  # in MB/s for EBS gp3
  additionalVolumes:
    - name: empty
      mountPath: /opt/empty
      targetContainers:
        - all
      volumeSource:
        emptyDir: {}
#    - name: data
#      mountPath: /home/postgres/pgdata/partitions
#      targetContainers:
#        - postgres
#      volumeSource:
#        PersistentVolumeClaim:
#          claimName: pvc-postgresql-data-partitions
#          readyOnly: false
#    - name: conf
#      mountPath: /etc/telegraf
#      subPath: telegraf.conf
#      targetContainers:
#        - telegraf-sidecar
#      volumeSource:
#        configMap:
#          name: my-config-map

  enableShmVolume: true
#  spiloRunAsUser: 101
#  spiloRunAsGroup: 103
#  spiloFSGroup: 103
#  podAnnotations:
#    annotation.key: value
#  serviceAnnotations:
#    annotation.key: value
#  podPriorityClassName: "spilo-pod-priority"
#  tolerations:
#  - key: postgres
#    operator: Exists
#    effect: NoSchedule
  resources:
    requests:
      cpu: 2000m
      memory: 8000Mi
    limits:
      cpu: 2000m
      memory: 8000Mi
  patroni:
    initdb:
      encoding: "UTF8"
      locale: "en_US.UTF-8"
      data-checksums: "true"
    pg_hba:
    - local      all          all                         trust
    - hostssl    all          +zalandos    127.0.0.1/32   pam
    - host       all          all          127.0.0.1/32   md5
    - host       all          all          172.17.0.1/16  md5
    - host       all          all          10.0.0.0/8     md5
    - hostssl    all          +zalandos    ::1/128        pam
    - host       all          all          ::1/128        md5
    - hostssl    replication  standby      all            md5
    - hostnossl  all          all          all            reject
    - hostssl    all          +zalandos    all            pam
    - hostssl    all          all          all            md5
#    - hostssl all all 0.0.0.0/0 md5
#    - host    all all 0.0.0.0/0 md5
#    slots:
#      permanent_physical_1:
#        type: physical
#      permanent_logical_1:
#        type: logical
#        database: foo
#        plugin: pgoutput
    ttl: 30
    loop_wait: &loop_wait 10
    retry_timeout: 10
    synchronous_mode: false
    synchronous_mode_strict: false
    maximum_lag_on_failover: 33554432

# restore a Postgres DB with point-in-time-recovery
# with a non-empty timestamp, clone from an S3 bucket using the latest backup before the timestamp
# with an empty/absent timestamp, clone from an existing alive cluster using pg_basebackup
#  clone:
#    uid: "efd12e58-5786-11e8-b5a7-06148230260c"
#    cluster: "acid-batman"
#    timestamp: "2017-12-19T12:40:33+01:00"  # timezone required (offset relative to UTC, see RFC 3339 section 5.6)
#    s3_wal_path: "s3://custom/path/to/bucket"

# run periodic backups with k8s cron jobs
#  enableLogicalBackup: true
#  logicalBackupSchedule: "30 00 * * *"

#  maintenanceWindows:
#  - 01:00-06:00  #UTC
#  - Sat:00:00-04:00

# overwrite custom properties for connection pooler deployments
#  connectionPooler:
#    numberOfInstances: 2
#    mode: "transaction"
#    schema: "pooler"
#    user: "pooler"
#    resources:
#      requests:
#        cpu: 300m
#        memory: 100Mi
#      limits:
#        cpu: "1"
#        memory: 100Mi

  initContainers:
  - name: date
    image: busybox
    command: [ "/bin/date" ]
#  sidecars:
#    - name: "telegraf-sidecar"
#      image: "telegraf:latest"
#      resources:
#        limits:
#          cpu: 500m
#          memory: 500Mi
#        requests:
#          cpu: 100m
#          memory: 100Mi
#      env:
#        - name: "USEFUL_VAR"
#          value: "perhaps-true"

# Custom TLS certificate. Disabled unless tls.secretName has a value.
  tls:
    secretName: ""  # should correspond to a Kubernetes Secret resource to load
    certificateFile: "tls.crt"
    privateKeyFile: "tls.key"
    caFile: ""  # optionally configure Postgres with a CA certificate
    caSecretName: "" # optionally the ca.crt can come from this secret instead.
# file names can be also defined with absolute path, and will no longer be relative
# to the "/tls/" path where the secret is being mounted by default, and "/tlsca/"
# where the caSecret is mounted by default.
# When TLS is enabled, also set spiloFSGroup parameter above to the relevant value.
# if unknown, set it to 103 which is the usual value in the default spilo images.
# In Openshift, there is no need to set spiloFSGroup/spilo_fsgroup.

# Add node affinity support by allowing postgres pods to schedule only on nodes that
# have label: "postgres-operator:enabled" set.
#  nodeAffinity:
#    requiredDuringSchedulingIgnoredDuringExecution:
#      nodeSelectorTerms:
#        - matchExpressions:
#            - key: postgres-operator
#              operator: In
#              values:
#                - enabled

