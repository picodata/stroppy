# Stroppy

- [Введение](#введение)
- [Основные возможности](#основные-возможности)
- [Запуск Stroppy](#запуск-stroppy)
  - [Варианты запуска](#варианты-запуска)
  - [Параметры terraform](#параметры-terraform)
  - [Файлы для kubespray](#файлы-для-kubespray)
  - [Манифесты БД и инфраструктуры](#манифесты-БД-и-инфраструктуры)
  - [Конфигурация теста](#конфигурация-теста)
  - [Запуск тестов](#запуск-тестов)
    - [Stroppy в docker](#stroppy-в-docker)
    - [Stroppy из исходников](#stroppy-из-исходников)
    - [Stroppy deploy](#stroppy-deploy)
- [Деплой Stroppy в minikube](#деплой-Stroppy-в-minikube)
  - [Подготовка окружения](#подготовка-окружения)
  - [Сборка Stroppy](#сборка-Stroppy)
- [Команды](#команды)
  - [Базовые ключи](#базовые-ключи)
  - [Ключи Stroppy deploy](#ключи-deploy)
  - [Базовые ключи pop и pay](#базовые-ключи-pop-и-pay)
  - [Базовые ключи chaos-тестов](#Базовые-ключи-chaos-тестов)
- [Сценарий тестирования](#сценарий-тестирования)
- [Модель данных](#модель-данных)
- [Управляемые неисправности](#управляемые-неисправности)
- [Особенности использования](#особенности-использования)

## Введение

Stroppy - это фреймворк для тестирования различных баз данных. Он позволяет 
развернуть кластер в облаке, запустить нагрузочные тесты и имитировать, например, 
сетевую недоступность одной из нод в кластере.  

Как же все это позволяет проверить надёжность? Дело в том, что для проверки 
целостности данных существует весьма элегантный "банковский" тест. Мы заполняем 
БД рядом записей о неких "счетах" с деньгами. Затем имитируем серию переводов 
с одного счета на другой в рамках предоставляемых СУБД транзакций. В результате 
любого количества транзакций общая сумма денег не должна измениться.  

Чтобы усложнить задачу для СУБД, Stroppy может попытаться сломать кластер БД, 
ведь в реальном мире отказы случаются гораздо чаще, чем нам хочется. А для 
горизонтально масштабируемых БД это случается еще чаще, так как большее количество 
физических узлов дает больше точек отказа.  

На данный момент мы реализовали поддержку FoundationDB, MongoDB, CockroachDB, 
YandexDB, PostgreSQL (нужно же с чем-то сравнивать всё остальное).  
Кроме того, для того, чтобы было удобнее анализировать результаты тестов, 
stroppy интегрирован с Grafana и после каждого прогона в автоматическом режиме 
собирает архив с графиками мониторинга, масштабированными по времени прогона. 
Также для FoundationDB и MongoDB поддерживается сбор внутренней статистики с 
заданной периодичностью - для FoundationDB собираются данные консольной команды 
status json, для MongoDB - данные команды db.serverStatus().  

> **Note:** Данная инструкция актуальна для использования на ОС Ubuntu >=18.04 
> и пока не проверялась на остальных операционных системах.


## Основные возможности  

- Развертывание кластера виртуальных машин в выбранном облаке через terraform. 
Поддерживается Yandex.Cloud и Oracle.Cloud.
- Развертывание kubernetes кластера в развернутом кластере виртуальных машин.
- Развертывание выбранной СУБД в этом кластере.
- Сбор статистики из Grafana метрик k8s кластера и системных метрик виртуальных 
машин (CPU, RAM, storage и т.д.).
- Управление параметрами тестов и самого развертывания - от кол-ва VM до п
одаваемой нагрузки и управляемых неполадок.
- Запуск тестов по команде из консоли
- Логирование хода теста - текущее и итоговое latency и RPS
- Удаление кластера виртуальных машин
- Развертывание нескольких кластеров с одной локальной машины с изолированным мониторингом и консолью запуска  

## Запуск Stroppy

Допустим, мы хотим проверить, какую нагрузку выдержит кластер FoundationDB, 
состоящий из 3 узлов по 1 ядру и 8 ГБ RAM на узел, при этом кластер будет 
развернут соответствующим [k8s оператором](https://github.com/FoundationDB/fdb-kubernetes-operator).

Запускать кластер мы будет в облаке Yandex CLoud.

---

### Варианты запуска

Что бы протестировать выбранную конфигурацию с помощью Stroppy мы можем 
пойти двумя разными путями:
- Запустить тесты вручную .
  - Развернуть виртуальные машины.
  - Развернуть кластер k8s и СУБД вручную.
  - Поднять рядом под со Stroppy из манифеста .
  - Смонтировать в под файл подключения к БД (если этого требует БД).
  - Смонтировать в под файл с конфигурацией теста.
  - Запустить загрузку счетов и затем тест переводов, воспользовавшись 
  командами из [Команды](#команды).
- Запустить тесты и развертывание автоматически.
  - При необходимости сконфигурировать инфраструктуру (или пропустить этот шаг)
  - Сконфигурировать параметры теста, или передать их в командной строке при запуске.

Далее, вне зависимости от выбранного варианта, нужно задать необходимые для
запуска параметры через соответствующие флаги командной строки. А так же 
файлы подготовить следующие файлы конфигурации.  

---

### Параметры terraform

Файл `third_party/terraform/vars.tf` используется `terraform` для чтения
переменных сценария. Изменение переменных в этом файле, приведет к увеличению 
или уменьшению количества виртуальных машин, ресурсов выделяемых на каждую 
виртуальную машину, количество `control plane` нод кластера, настройки 
сервисного аккаунта, и т.д.

Например, такая конфигурация приведет к созданию кластера с тремя kubelet
нодами у каждой из которых будет по 2 CPU и 8 GB ОЗУ.
```terraform
variable "workers_count" {
    type = number
    description = "Yandex Cloud count of workers"
    default = 3
}

variable "workers_cpu" {
    type = number
    description = "Yandex Cloud cpu in cores per worker"
    default = 2
}

variable "workers_memory" {
    type = number
    description = "Yandex Cloud memory in GB per worker"
    default = 8
}
```

Так же, не обязательно задавать параметры именно в `vars.tf`, можно 
просто задать перемнные окружения которые будут прочитаны `terraform`.
```shell
export TF_VAR_workers_count=3
export TF_VAR_workers_cpu=2
export TF_VAR_workers_memory=8
```

---
> **Note:** Список переменных можно посмотреть в [tfvars](third_party/terraform/tfvars)
---

Файл `third_party/terraform/main.tf` - скрипт по созданию инфраструктуры для 
`terraform`. Не обязателен. `Stroppy` умеет сам создавать сценарии `hcl`.
Но! Если в процессе деплоя строппи обнаружит скрипт, то он применит именно
его, не меняя ничего в структуре скрипта.

---
> **Note:**: Обратите внимаение, что VM в Oracle.Cloud, как и аналогичные ему,
> использует процессоры с мультитредингом, а k8s, в таком случае, при оценке
> заданных limits и requests ориентируется на количество виртуальных ядер 
> (тредов), а не на физические. Поэтому указав cpu:2, мы фактически получили 4
> виртуальных ядра, 2 из которых отдадим FoundationDB.
---

### Файлы для kubespray

При желании вы можете управлять деплоем k8s с момощью изменения файлов для
`ansible` находящихся в директории `third_party/kubespray`

---

### Манифесты БД и инфраструктуры

В директории `third_party/extra/manifests` находятся манифесты для k8s которые 
будут использованы в процессе деплоя. При желании их можно изменять.

Манифесты для деплоя тестируемых баз данных находятся в директории 
`third_party/extra/manifests/databases`.

---

### Конфигурация теста

Файл `third_party/tests/test_config.json` содержит параметры для тестов БД. 
Пример файла ниже. Название и назначение параметров совпадает с параметрами 
для запуска тестов из раздела [Команды](#команды).

```json
{
  "log_level": "info", 
  "banRangeMultiplier": 1.1,
  "database_type": [ 
    "fdb" 
  ],
  "cmd": [
    {
      "pop": { // здесь задаем параметры теста загрузки счетов
        "count": 5000 
      }
    },
    {
      "pay": { // здесь задаем параметры теста переводов
        "count": 100000, 
        "zipfian": false, 
        "oracle": false, 
        "check": true 
      }
    }
  ]
}
```

---
> **Note:** Файл не обязателен, и будет использован  `Stroppy` только в том 
> случае если какие то параметры будут отличаться от настроек по умолчанию.
---

### Запуск тестов

После того как мы (если это было необходимо) задали конфигурацию 
инфраструктуры, модицифировали манифесты БД, отредактировали параметры теста, 
необходимо выбрать как именно мы хотим запустить сам  `Stroppy`.

---

#### Stroppy в docker

1) Запускаем уже готовый `docker` образ с клиентом  `Stroppy` 
```shell
docker run -it docker.binary.picodata.io/stroppy:latest
```
2) Задаем для доступа к выбранному нами облаку необходимые `terraform` для 
создание инфраструктуры.
- yandex
```shell
TF_VAR_token=
TF_VAR_cloud_id=
TF_VAR_folder_id=
TF_VAR_zone=
TF_VAR_network_id=
```

- oracle
```shell
TF_VAR_tenancy_ocid=
TF_VAR_user_ocid=
TF_VAR_region=
```

3) Запускаем  `Stroppy` с параметрами для тестируемой базы данных
```shell
stroppy deploy --cloud yandex --dbtype fdb
```

#### Stroppy из исходников

1) Клонируем себе репозиторий на локальную машину:
```shell
git clone https://github.com/picodata/stroppy.git
```

2) Устанавливаем клиент FoundationDB
```shell
curl -fLO https://github.com/apple/foundationdb/releases/download/7.1.25/foundationdb-clients_7.1.25-1_amd64.deb
sudo dpkg -i foundationdb-clients_7.1.25-1_amd64.deb
```

3) Устанавливаем `Ansible`
```shell
sudo apt update
sudo apt install -y python ansible
```

4) Устанавливаем `terraform`
- Добавляем gpg ключ hashicorp
```shell
wget -O- https://apt.releases.hashicorp.com/gpg | \
    gpg --dearmor | \
    sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg
```
- Добавляем репозитория
```shell
echo "deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] \
    https://apt.releases.hashicorp.com $(lsb_release -cs) main" | \
    sudo tee /etc/apt/sources.list.d/hashicorp.list
```
- Обновляем кэш `apt` и устанавливаем `terraform`
```shell
sudo apt update
sudo apt install -y terraform
```

5) Переходим в директорию со  `Stroppy` и устанавливаем зависимости для корректной работы `kubespray`
```shell
cd stroppy
python3 -m venv stroppy
source stroppy/bin/activate
python3 -m pip install -f third_party/kubespray/requirements.txt
```

6) Устанаваливаем `kubectl`.
```shell
curl -fsLO https://storage.googleapis.com/kubernetes-release/release/v1.25.0/bin/linux/amd64/kubectl
sudo chmod +x kubectl
sudo mv kubectl /usr/bin/kubectl
```

7) Запускаем сборку `Stroppy`.
```shell
sudo go build -o /usr/bin/stroppy ./cmd/stroppy
```

8) Запускаем `Stroppy` с параметрами для тестируемой базы данных.
```shell
stroppy deploy --cloud yandex --dbtype fdb
```

---

#### Stroppy deploy

Вне зависимости от того запускаем ли мы строппи в docker или локально собрав из
исходников, аргументы которые мы можем передать при запуске одни и те же. 
Например:

```shell
stroppy deploy --cloud yandex --dbtype fdb
```

```shell
stroppy deploy --cloud oracle --dbtype fdb 
```

---
> **Note:** Описание ключей команд можно найти в разделе - [Команды](#команды)
---

Для того что бы задеплоить кластер кубернетеса в облачной
инфраструктуре в корневой директории с проектом должны быть:  

**Yandex.Cloud:**  
- Приватный и публичный ключи. Обязательно именовать их как
`id_rsa` и `id_rsa.pub` во избежание проблем. Создать ключи можно при
командой `ssh-keygen -b 4096 -f id_rsa`.  
- Файл с credentials и атрибутами для доступа к cloud, лучше назвать его
`vars.tf`, для гарантированной совместимости.  
 
**Oracle.Cloud:**  
- Приватный ключ `private_key.pem`. Данный ключ необходимо получить с
помощью веб-интерфейса провайдера.

---

После вывода в консоль некоторого кол-ва отладочной информации и прошествию 
примерно 10-20 минут результатом выполнения команды должно стать сообщение вида:
```sh
Started ssh tunnel for kubernetes cluster and port-forward for monitoring.
To access Grafana use address localhost:3000.
To access to kubernetes cluster in cloud use address localhost:6443.
Enter "quit" or "exit" to exit Stroppy and destroy cluster.
Enter "pop" to start populating deployed DB with accounts.
Enter "pay" to start transfers test in deployed DB.
To use kubectl for access kubernetes cluster in another console
execute command for set environment variables KUBECONFIG before using:
"export KUBECONFIG=$(pwd)/config"
>                               
```

Строка приглашения в виде `>` означает что развертываение инфраструктуры, 
деплой мониторинга и базы данных прошли успешно. Наш кластер готов к 
тестированию. Под сообщением будет открыта консоль для выбора команд. 
Для старта теста загрузки счетов необходимо ввести команду `pop` и 
дождаться выполнения. Результатом успешного выполнения теста будет примерно 
следующий вывод:

```shell
[Nov 17 15:23:07.334] Done 10000 accounts, 0 errors, 16171 duplicates 
[Nov 17 15:23:07.342] dummy chaos successfully stopped             
[Nov 17 15:23:07.342] Total time: 21.378s, 467 t/sec               
[Nov 17 15:23:07.342] Latency min/max/avg: 0.009s/0.612s/0.099s    
[Nov 17 15:23:07.342] Latency 95/99/99.9%: 0.187s/0.257s/0.258s    
[Nov 17 15:23:07.344] Calculating the total balance...             
[Nov 17 15:23:07.384] Persisting the total balance...              
[Nov 17 15:23:07.494] Total balance: 4990437743 
```

---
> **Note:** Указанные порты в интерактивном режиме (режим Stroppy в которым
> он ожидает ввод следующей команды) это порты по умолчанию для доступа к
> мониторингу (порт 3000) и доступа к API k8s кластера(6443). Т.к. Stroppy
> поддерживает развертывание нескольких кластеров на одной локальной машине,
> то порты для кластеров, запущенных после первого, будут инкрементироваться.
---

После выполнения `pop` мы вновь увидим консоль для ввода команд и сможем 
вввести команду `pay` для старта теста переводов. `Pay` тест будет запущены с 
теми параметрами, которые мы задали на этапе конфигурирования в файле 
`test_config.json`, или в аргументах при запуске `deploy`.
Примером успешного выполнения будет вывод примерно следующего содержания:

```shell
[Nov 17 15:23:07.334] Done 10000 accounts, 0 errors, 16171 duplicates 
[Nov 17 15:23:07.342] dummy chaos successfully stopped             
[Nov 17 15:23:07.342] Total time: 21.378s, 467 t/sec               
[Nov 17 15:23:07.342] Latency min/max/avg: 0.009s/0.612s/0.099s    
[Nov 17 15:23:07.342] Latency 95/99/99.9%: 0.187s/0.257s/0.258s    
[Nov 17 15:23:07.344] Calculating the total balance...             
[Nov 17 15:23:07.384] Persisting the total balance...              
[Nov 17 15:23:07.494] Total balance: 4990437743 
```

---
> **Note:** Для варианта с тестированием FoundationDB, который мы запланировали 
> после успешного деплоя и вывода сообщения необходимо выполнить немного ручных 
> манипуляций из пункта 2 раздела "Особенности использования".
---

### Результаты теста

Результатом работы команд будет несколько файлов в корне директории с 
конфигурацией. Например, для нашего случая:

```pop_test_run_2021-10-15T16:09:51+04:00.log``` - файл с логами теста загрузки счетов  
```pay_test_run_2021-10-15T16:10:46+04:00.log``` - файл с логами теста переводов  
```monitoring/grafana-on-premise/fdb_pop_5000_1.1_zipfian_false_2021-10-15T16_10_46.tar.gz``` - архив с метриками теста загрузки счетов  
```monitoring/grafana-on-premise/fdb_pay_5000_1.1_zipfian_false_2021-10-15T16_10_46.tar.gz``` - архив с метриками теста переводов  

Если вместо сообщения в консоли возникает ошибка, которая не устраняется 
перезапуском (не более 3 повторов), то заводим issue с описанием ошибки в 
<https://github.com/picodata/stroppy/issues>. Повторить тест можно повторным 
запуском сначала `pop` а потом `pay`.

Переповтор идемпотентен для кластера VM и кластера K8S, поэтому при 
переповторе не будут созданы новые виртуальные машины и кластер Kubernetes.

> **Note** Stroppy пока не гарантирует идемпотентность в отношении развертывания 
> выбранной СУБД. Такое поведение оставлено без изменений в том числе, чтобы дать 
> возможность исправить ошибку конфигурирования БД без редеплоя всего кластера.

## Деплой Stroppy в minikube

Для локального тестирования каких либо новых фич, (или просто для того что-бы 
попробовать) `Stroppy` поддерживает запуск в `Minikube`.

### Подготовка окружения

1. Устанавливаем `Minikube`.
```sh
curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube
sudo mkdir -p /usr/local/bin/
sudo install minikube /usr/local/bin/
minikube version
```

2. Устанавливаем `kubectl`
```sh
curl -fsLO https://storage.googleapis.com/kubernetes-release/release/v1.25.0/bin/linux/amd64/kubectl
sudo chmod +x kubectl
sudo mv kubectl /usr/bin/kubectl
```

3. Настраиваем `Minikube`.
```sh
minikube config set memory 6144
minikube config set cpus 4
```

4. Запускаем `Minikube`.
```sh
minikube start
```

### Сборка Stroppy

1. Клонируем репозиторй `Stroppy` и произведём подготовку к разворачиванию.
```shell
git clone https://github.com/picodata/stroppy.git && cd stroppy
make all
```

2. Стартуем кластер, в данном случае мы используем `Postgres`.
```shell
kubectl apply -f third_party/extra/manifests/stroppy/deployment.yml
kubectl apply -r -f third_party/extra/manifests/databases/postgres
```

3. Проверяем как поднялся кластер, все ли поды перешли в состояние Running.
```shell
minikube status && kubectl get pods && kubectl cluster-info
```
Если всё с кластером хорошо и он работает, должны увидеть нечто подобное
```
minikube
type: Control Plane
host: Running
kubelet: Running
apiserver: Running
kubeconfig: Configured

NAME                                READY   STATUS    RESTARTS   AGE
acid-postgres-cluster-0             1/1     Running   0          15m
acid-postgres-cluster-1             1/1     Running   0          14m
acid-postgres-cluster-2             1/1     Running   0          14m
postgres-operator-c8d5c8649-jqlbf   1/1     Running   0          16m
stroppy-client                      1/1     Running   0          16m
Kubernetes control plane is running at https://192.168.49.2:8443
CoreDNS is running at https://192.168.49.2:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
```

4. Подключаемся к контейнеру `Stroppy`.
```shell
kubectl exec --stdin --tty stroppy-client -- /bin/bash
```

5. Запускаем тестирование.
```shell
stroppy pop --url postgres://stroppy:stroppy@acid-postgres-cluster/stroppy?sslmode=disable --count 5000 --run-type client --kube-master-addr=8.8.8.8  --dir .
stroppy pay --url postgres://stroppy:stroppy@acid-postgres-cluster/stroppy?sslmode=disable --check --count=100000 --run-type client --kube-master-addr=8.8.8.8  --dir .
```

---
## Команды

Что бы эффективно использовать строппи необходимо изучить набор доступных
комманд и опций.

---

### Базовые ключи

`run-type` - тип запуска  `Stroppy`. Если не нужно развертывание
инфраструктуры, то опцию можно не указывать. Для запуска Stroppy в режиме
клиента нужно указать `client`. Для запуска интеграционных тестов `local`.
`log-level` - уровень логирования. Поддерживается trace, debug, info, 
warn, error, fatal, panic;  
`dbtype` - наименование тестируемой СУБД. Поддерживается postgres 
(PostgreSQL), fdb(FoundationDB), mongodb(MongoDB), cocroach(cockroach);  
`url` - строка подключения к тестируемой БД.

---

### Ключи deploy

`cloud` - имя выбранного облачного провайдера. Поддерживается `yandex` и 
`oracle`.  
`dir` - директория с файлами конфигурации. По умолчанию текущая директория.

**Пример запуска развертывания кластера в облаке**:

```sh
stroppy deploy --cloud oracle --dir . --log-level debug
```

---

### Базовые ключи pop и pay

`count, n` - кол-во загружаемых счетов, по умолчанию 100000;  
`workers, w` - кол-во воркеров нагрузки (потоков-горутин), по умолчанию 
`4 * runtime.NumCPU()`;  
`banRangeMultiplier, r` - коэффициент, определяющий сооотношение BIC/BAN 
в процессе генерации, подробности ниже;  
`stat-interval, s` - интервал сбора статистики, по умолчанию 10 секунд;  
`pool-size` - размер пула соединений к БД. Актуально для PostgreSQL, MongoDB и 
CocroachDB. Если ключ не задан, то размер пула равен кол-ву воркеров.
Для PostgreSQL и CocroachDB размер пула также может быть задан через параметр 
`max_pool_size` в строке подключения. В этом случае параметр `pool-size`
игнорируется.

***Важное замечание***:

`banRangeMultiplier` (далее brm) - это число, определяющее соотношение BAN 
(Идентификационный номер банка) к BIC (Идентификационный код банка).
Количество сгенерированных битов приблизительно равно квадратному корню из 
кол-ва счетов (параметр `count`).  
Количество BAN определяется по следующей формуле:  
`Nban = (Nbic *brm)/square(count)`.  
Если Nban* Nbic > count, мы генерируем больше комбинаций (BIC, BAN), чем мы 
сохранили во время процесса заполнения БД (это достигается, если brm > 1).  
Рекомендуемый диапазон brm составляет от 1,01 до 1,1. Увеличение снижает кол-во 
not found на тесте переводов, но увеличивает кол-во dublicates на этапе 
загрузки счетов.  
Значение по умолчанию для параметра banRangeMultipluer равно 1.1.

**Пример команды запуска теста загрузки счетов**:

```sh
stroppy pop --run-type client --url fdb.cluster --count 5000 --w 512 --dbtype=fdb
```  

Дополнительные ключи для команды `pop`:  
`sharded` - флаг использования шардирования при создании схемы данных. 
Актуально только для MongoDB, по умолчанию false;

**Пример команды запуска теста переводов**:

```sh
stroppy pay --run-type client --url fdb.cluster --check --count=100000
```

Дополнительные ключи для команды `pay`:  
`zipfian` - флаг использования распределения данных по закону Ципфа, 
по умолчанию `false`.  
`oracle` - флаг внутренней проверки переводов. Пока не используется, 
указан для совместимости для `oracle`.  
`check` - флаг проверки результатов теста. Суть проверки - подсчет 
суммарного баланса счетов после теста и сравнение этого значения с сохраненным 
суммарным балансом после теста загрузки счетов. По умолчанию `true`.  

---

### Базовые ключи chaos тестов

`kube-master-addr` - внутренний ip-адрес мастер-ноды развернутого 
kubernetes-кластера.  
`chaos-parameter` - имена файлов сценариев chaos-mesh, расположенных в 
папке `deploy/databases/имя тестируемой СУБД/chaos`. Указывается без 
расширения .yaml

---

## Сценарий тестирования

Для того чтобы иметь возможность проверять как корректность менеджера 
транзакций, так и его производительность, нагрузочный тест имитирует серию 
банковских денежных переводов между счетами. Ключевая идея, которая делает 
этот тест полезным для проверки целостности данных без прибегания к оракулу 
(то есть, без сравнения с каноническим результатом), заключается в том, что 
никакие денежные переводы не могут изменить общий баланс по всем счетам.  
Таким образом, тест состоит из трех основных этапов:

1) Загрузка счетов. Общий баланс рассчитывается и сохраняется отдельно как 
канонический / ожидаемый результат.

Для создания записей используется самописный генератор, который с течением 
времени может производить дубликаты в рамках теста. Но вставка счетов 
реализована таким образом, чтобы в БД сохранялись только уникальные записи 
и кол-во успешно загруженных записей совпадало с заданным.

2) Серия денежных переводов между счетами. Переводы выполняются параллельно 
и могут использовать одну и ту же исходную или целевую учетную запись.

3) Подсчет суммарного баланса счетов и его сравнение с общим балансом, 
полученным на этапе загрузки счетов.

Пример лога успешного завершения теста загрузки счетов:

```sh
[Nov 17 15:23:07.334] Done 10000 accounts, 0 errors, 16171 duplicates 
[Nov 17 15:23:07.342] dummy chaos successfully stopped             
[Nov 17 15:23:07.342] Total time: 21.378s, 467 t/sec               
[Nov 17 15:23:07.342] Latency min/max/avg: 0.009s/0.612s/0.099s    
[Nov 17 15:23:07.342] Latency 95/99/99.9%: 0.187s/0.257s/0.258s    
[Nov 17 15:23:07.344] Calculating the total balance...             
[Nov 17 15:23:07.384] Persisting the total balance...              
[Nov 17 15:23:07.494] Total balance: 4990437743 
```

Пример лога успешного завершения теста переводов:

```sh
[Oct 15 16:11:12.872] Total time: 26.486s, 377 t/sec             
[Oct 15 16:11:12.872] Latency min/max/avg: 0.001s/6.442s/0.314s    
[Oct 15 16:11:12.872] Latency 95/99/99.9%: 0.575s/3.268s/6.407s    
[Oct 15 16:11:12.872] dummy chaos successfully stopped             
[Oct 15 16:11:12.872] Errors: 0, Retries: 0, Recoveries: 0, Not found: 1756, Overdraft: 49 
[Oct 15 16:11:12.872] Calculating the total balance...             
[Oct 15 16:11:12.922] Final balance: 4930494048 
```

Пример окончания лога в случае расхождения итогового баланса:

```sh
Calculating the total balance...             
Check balance mismatch:
before: 748385757108.0000000000
after:  4999928088923.9300000000
```

В процессе выполнения тестов воркеры Stroppy могут получать различные ошибки 
из-за проблем инфраструктуры или состояния СУБД. Для обеспечения устойчивости 
теста воркер, получивший ошибку из некоторого пула ошибок, выявленных на этапе 
отладки и тестов, останавливается на некоторый период (до 10 миллисекунд), 
увеличивает счетчик ```Retries``` - кол-во повторов, и выполняет операцию с 
новыми сгенерированным счетом. Пул состоит как из общих ошибок, так и 
специфических для тестируемой СУБД. Для изучения списка рекомендуется 
о братиться в [пакет payload](https://github.com/picodata/stroppy/tree/main/internal/payload).
Если воркер получает ошибку, которой нет в пуле, он останавливает свою работу 
с выводом в лог фатальной ошибки и увеличением счетчика ```Errors```.

Также внутри Stroppy определено несколько счетчиков для "логических" ошибок, 
которые является штатным поведением в общем смысле, но фиксируются отдельно 
от общего кол-ва операций:

`dublicates` - кол-во операций, получивших ошибку дублирования данных. 
Актуально для теста загрузки счетов.  
`Not found` - кол-во операций, завершившихся с ошибкой по причине того, 
что запись с переданным счетов не найдена в БД. Актуально для теста переводов.  
`Overdraft` - кол-во операций, завершившихся с ошибкой по причине того, 
что баланс счета-источника недостаточен для перевода с переданной суммой. 
Т.е. Stroppy не выполняет перевод, который может увести баланс 
счета-источника в минус.

---

## Модель данных

На примере PostgreSQL:

| Таблица   |   Столбец   | Значение и тип данных                     |
|-----------|:-----------:|:------------------------------------------|
| accounts  |     bic     | BIC счета, TEXT                           |
|           |     ban     | BAN счета, TEXT                           |
|           |   balance   | баланс счета, DECIMAL                     |
| transfers | transfer_id | идентификатор перевода, UUID              |
|           |   src_bic   | BIC счёта отправителя, TEXT               |
|           |   src_ban   | BAN счёта отправителя, TEXT               |
|           |   dst_bic   | BIC счёта получателя, TEXT                |
|           |   dst_ban   | BAN счёта получателя, TEXT                |
|           |   amount    | сумма перевода, DECIMAL                   |
| checksum  |    name     | имя для хранения итогового баланса, TEXT  |
|           |   amount    | значение итогового баланса, DECIMAL       |
| settings  |     key     | наименование настроечного параметра, TEXT |
|           |    value    | значение настроечного параметра, TEXT     |

Первичным ключом таблицы accounts является пара значений BIC и BAN, таблицы 
transfer - transfer_id, значение которого генерируется пакетом 
[github.com/google/uuid](github.com/google/uuid). Для других СУБД используется 
аналогичная модель данных с учетом нюансов реализации самой СУБД. Также стоит 
отметить, что для PostgreSQL и MongoDB в методе, который выполняет перевод, 
реализовано управление порядком блокировок для исключения дедлоков. 
Управление осуществляется путем лексикографического сравнения пар BIC и BAN 
счета-источника и счета-получателя.

---

## Управляемые неисправности

Использование управляемых неисправностей в Stroppy реализовано с помощью 
[chaos-mesh](https://chaos-mesh.org/) - решения для управления хаос-тестами, 
которое вводит ошибки на каждом уровне системы Kubernetes.

**Пример запуска теста с использованием сценария chaos-mesh**:

```sh
stroppy pay --run-type client --url fdb.cluster --check --count=100000 --kube-master-addr=10.1.20.109 --chaos-parameter=fdb-cont-kill-first
```

---

## Особенности использования

1. Запуск в Oracle.Cloud и Yandex.Cloud имеет различия:

- для деплоя трех машин-воркеров и одного мастера в yandex.cloud указываем 
nodes=3, в Oracle.Cloud = 4, т.е. для деплоя в Oracle Cloud мастер учитывается 
в кол-ве создаваемых нод, в случае с Yandex.Cloud создается по умолчанию.

- в деплое Oracle.Cloud есть дополнительный шаг - монтирование отдельных 
network storages по протоколу ISCSI. В Yandex.Cloud используются локальные 
диски виртуальных машин.

> **Note:** Oracle.Cloud имеет особенность, причины которой пока не 
> установлены: при ручном удалении кластера через GUI нужно явно удалить 
> block volumes в соответствующем разделе. Вместе с кластером они 
> могут НЕ УДАЛИТЬСЯ!!!

2. Для запуска тестов FoundationDB предварительно необходимо скопировать 
содержимое файла или сам файл fdb.cluster, расположенный в директории 
`/var/dynamic-conf` внутри пода sample-cluster-client (имя пода может 
иметь дополнительный цифро-буквенный постфикс), и  вставить его в директорию 
`/root/` внутри пода Stroppy-client. Это необходимо для доступа к кластеру 
и, на текущий момент, пока не автоматизировано.

3. Архив с графиками мониторинга создается на локальной машине, в директории 
monitoring/grafana-on-premise каталога с конфигурационными файлами. Среднее 
время создания архива - 30 минут (для Yandex больше, для Oracle меньше). 
Создание архива происходит после окончания любого из тестов.

4. Статистика status json для FoundationDB собирается в файл, который 
лежит внутри пода Stroppy в кластере k8s, в директории `/root/`, имя файла 
генерится по маске status_json_время_старта_cбора_статистики.json. Сбор
статистики запускается перед тестом и завершается вместе с его окончанием. 
Пока сбор статистики реализован только для FoundationDB, в дальнейшем 
может быть реализована поддержка сбора специфической статистики для 
других СУБД. Файлы статистики хранятся внутри пода Stroppy, их 
копирование на рабочую машину пока не автоматизировано.

5. Для развертывания нескольких кластеров в облаке с одной локальной 
машины рекомендуется сделать несколько копий репозитория Stroppy со своими 
директории файлов конфигурации. Это позволит избежать наложений и гибко 
управлять каждым из кластеров.
