apiVersion: "acid.zalan.do/v1"
kind: postgresql
metadata:
  name: acid-postgres-cluster
#  labels:
#    environment: demo
#  annotations:
#    "acid.zalan.do/controller": "second-operator"
#    "delete-date": "2020-08-31"  # can only be deleted on that day if "delete-date "key is configured
#    "delete-clustername": "acid-test-cluster"  # can only be deleted when name matches if "delete-clustername" key is configured
spec:
  dockerImage: registry.opensource.zalan.do/acid/spilo-13:2.0-p2
  teamId: "acid"
  numberOfInstances: 2
  users:  # Application/Robot users
    zalando:
    - superuser
    - createdb
  enableMasterLoadBalancer: false
  enableReplicaLoadBalancer: false
  enableConnectionPooler: false # enable/disable connection pooler deployment
  enableReplicaConnectionPooler: false # set to enable connectionPooler for replica service
  allowedSourceRanges:  # load balancers' source ranges for both master and replica services
  - 127.0.0.1/32
  databases:
    foo: zalando
  preparedDatabases:
    bar:
      defaultUsers: true
      extensions:
        pg_partman: public
        pgcrypto: public
      schemas:
        data: {}
        history:
          defaultRoles: true
          defaultUsers: false
  postgresql:
    version: "13"
    parameters:  # Used generator: http://pgconfigurator.cybertec.at
      # Connectivity
      max_connections: '100'
      superuser_reserved_connections: '3'

      # Memory Settings
      shared_buffers: '256 MB'
      work_mem: '32 MB'
      maintenance_work_mem: '320 MB'
      huge_pages: 'off'
      effective_cache_size: '1 GB'
      effective_io_concurrency: '100'   # concurrent IO only really activated if OS supports posix_fadvise function
      random_page_cost: '1.25' # speed of random disk access relative to sequential access (1.0)

      # Monitoring
      shared_preload_libraries: 'pg_stat_statements'    # per statement resource usage stats
      track_io_timing: 'on'        # measure exact block IO times
      track_functions: pl        # track execution times of pl-language procedures if any

      # Replication
      wal_level: replica		# consider using at least 'replica'
      max_wal_senders: '0'
      synchronous_commit: 'on'

      # Checkpointing: 
      checkpoint_timeout : '15 min' 
      checkpoint_completion_target: '0.9'
      max_wal_size: '1024 MB'
      min_wal_size: '512 MB'

      # WAL writing
      wal_compression: 'on'
      wal_buffers: '-1'    # auto-tuned by Postgres till maximum of segment size (16MB by default)
      wal_writer_delay: 200ms
      wal_writer_flush_after: 1MB

      # Background writer
      bgwriter_delay: 200ms
      bgwriter_lru_maxpages: '100'
      bgwriter_lru_multiplier: '2.0'
      bgwriter_flush_after: '0'

      # Parallel queries: 
      max_worker_processes: '1'
      max_parallel_workers_per_gather: '0'
      max_parallel_maintenance_workers: '0'
      max_parallel_workers: '1'
      parallel_leader_participation: 'on'

      # Advanced features 
      enable_partitionwise_join: 'on'
      enable_partitionwise_aggregate: 'on'
      jit: 'on'
  volume:
    size: 1Gi
#    storageClass: my-sc
#    iops: 1000  # for EBS gp3
#    throughput: 250  # in MB/s for EBS gp3
  additionalVolumes:
    - name: empty
      mountPath: /opt/empty
      targetContainers:
        - all
      volumeSource:
        emptyDir: {}
#    - name: data
#      mountPath: /home/postgres/pgdata/partitions
#      targetContainers:
#        - postgres
#      volumeSource:
#        PersistentVolumeClaim:
#          claimName: pvc-postgresql-data-partitions
#          readyOnly: false
#    - name: conf
#      mountPath: /etc/telegraf
#      subPath: telegraf.conf
#      targetContainers:
#        - telegraf-sidecar
#      volumeSource:
#        configMap:
#          name: my-config-map

  enableShmVolume: true
#  spiloRunAsUser: 101
#  spiloRunAsGroup: 103
#  spiloFSGroup: 103
#  podAnnotations:
#    annotation.key: value
#  serviceAnnotations:
#    annotation.key: value
#  podPriorityClassName: "spilo-pod-priority"
#  tolerations:
#  - key: postgres
#    operator: Exists
#    effect: NoSchedule
  resources:
    requests:
      cpu: 1000m
      memory: 1000Mi
    limits:
      cpu: 1000m
      memory: 1000Mi
  patroni:
    initdb:
      encoding: "UTF8"
      locale: "en_US.UTF-8"
      data-checksums: "true"
#    pg_hba:
#      - hostssl all all 0.0.0.0/0 md5
#      - host    all all 0.0.0.0/0 md5
#    slots:
#      permanent_physical_1:
#        type: physical
#      permanent_logical_1:
#        type: logical
#        database: foo
#        plugin: pgoutput
    ttl: 30
    loop_wait: &loop_wait 10
    retry_timeout: 10
    synchronous_mode: false
    synchronous_mode_strict: false
    maximum_lag_on_failover: 33554432

# restore a Postgres DB with point-in-time-recovery
# with a non-empty timestamp, clone from an S3 bucket using the latest backup before the timestamp
# with an empty/absent timestamp, clone from an existing alive cluster using pg_basebackup
#  clone:
#    uid: "efd12e58-5786-11e8-b5a7-06148230260c"
#    cluster: "acid-batman"
#    timestamp: "2017-12-19T12:40:33+01:00"  # timezone required (offset relative to UTC, see RFC 3339 section 5.6)
#    s3_wal_path: "s3://custom/path/to/bucket"

# run periodic backups with k8s cron jobs
#  enableLogicalBackup: true
#  logicalBackupSchedule: "30 00 * * *"

#  maintenanceWindows:
#  - 01:00-06:00  #UTC
#  - Sat:00:00-04:00

# overwrite custom properties for connection pooler deployments
#  connectionPooler:
#    numberOfInstances: 2
#    mode: "transaction"
#    schema: "pooler"
#    user: "pooler"
#    resources:
#      requests:
#        cpu: 300m
#        memory: 100Mi
#      limits:
#        cpu: "1"
#        memory: 100Mi

  initContainers:
  - name: date
    image: busybox
    command: [ "/bin/date" ]
#  sidecars:
#    - name: "telegraf-sidecar"
#      image: "telegraf:latest"
#      resources:
#        limits:
#          cpu: 500m
#          memory: 500Mi
#        requests:
#          cpu: 100m
#          memory: 100Mi
#      env:
#        - name: "USEFUL_VAR"
#          value: "perhaps-true"

# Custom TLS certificate. Disabled unless tls.secretName has a value.
  tls:
    secretName: ""  # should correspond to a Kubernetes Secret resource to load
    certificateFile: "tls.crt"
    privateKeyFile: "tls.key"
    caFile: ""  # optionally configure Postgres with a CA certificate
    caSecretName: "" # optionally the ca.crt can come from this secret instead.
# file names can be also defined with absolute path, and will no longer be relative
# to the "/tls/" path where the secret is being mounted by default, and "/tlsca/"
# where the caSecret is mounted by default.
# When TLS is enabled, also set spiloFSGroup parameter above to the relevant value.
# if unknown, set it to 103 which is the usual value in the default spilo images.
# In Openshift, there is no need to set spiloFSGroup/spilo_fsgroup.

# Add node affinity support by allowing postgres pods to schedule only on nodes that
# have label: "postgres-operator:enabled" set.
#  nodeAffinity:
#    requiredDuringSchedulingIgnoredDuringExecution:
#      nodeSelectorTerms:
#        - matchExpressions:
#            - key: postgres-operator
#              operator: In
#              values:
#                - enabled
